<!DOCTYPE html>
<html lang="en">
    <head>
        <title>Lucas Waldburger</title>
        <meta property="og:title" content="Lucas Waldburger" />
        <meta property="og:image" content="https://haoliu.site/files/img/hao.png" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="author" content="Lucas Waldburger">
        <meta name="viewport" content="width=device-width initial-scale=1" />
        <meta name="description" content="Homepage of Lucas Waldburger PhD student at UC Berkeley">
        <meta charset="utf-8">
    
        <link rel="stylesheet" type="text/css" href="files/css/style.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css"
            integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
        <script src="https://code.jquery.com/jquery-3.7.0.slim.min.js"
            integrity="sha256-tG5mcZUtJsZvyKAxYLVXrmjKBVLd6VpVccqz/r4ypFE=" crossorigin="anonymous"></script>
    </head>
    
    <body>
        <style>
            pre {
                text-align: left;
                white-space: pre-line;
              }
            a:link {
                text-decoration: underline;
            }
        </style>
    
        <table>
            <tr>
                <td><img src="files/img/waldburger.jpeg" height="200" alt="lucas_waldburger" style="padding: 20px;"></td>
                <td>
                    <div style="font-size:22; font-weight:bold">Lucas Waldburger</div>
                    <div>PhD student at UC Berkeley<br/></div>
                    <div>
                        <tt>&#108;&#119;&#97;&#108;&#100;&#98;&#117;&#114;&#103;&#101;&#114;&#64;&#98;&#101;&#114;&#107;&#101;&#108;&#101;&#121;&#46;&#101;&#100;&#117;</tt> <br/>
                    </div>
                    <style>.link {font-size: 14px}</style>
                    <div>
                        <a href="https://scholar.google.co.uk/citations?user=v26ZPFEAAAAJ&hl=en" class="link">Google Scholar</a>,
                        <a href="https://github.com/lucaswaldburger" class="link">Github</a>,
                        <a href="https://twitter.com/lucaswaldburger" class="link">Twitter</a>
                    </div>
                </td>
            </tr>
        </table>
    
        <div id="bio" class="section">
            <span class="header"></span><b>About</b></span>
            <div style="padding-bottom: 5px;">
                I am a PhD student in Bioengineering at UC Berkeley advised by Prof. Jay Keasling and Prof. Patrick Shih.
            </div>
    
            <div style="padding-bottom: 5px;">
                My research area is in using machine learning and functional genomics to explore biosynthetic diversity and engineer complex synthetic functions in living systems.
            </div>
        </div>
<!--     
    
        <div id="research" class="section">
            <span class="header">Recent and selected publication</span>
    
            <script>
            let activeItem = null;
            function toggleItem(item) {
                if (activeItem !== null) {
                    $('#' + activeItem).hide();
                }
                if (activeItem === item) {
                    activeItem = null;
                } else {
                    $('#' + item).show();
                    activeItem = item;
                }
            }
            </script>
    
            <style>
                strong {font-size: 15.5px; font-weight: 500;}
            </style>
    
            <li class="paper"> <strong>Ring Attention with Blockwise Transformers for Near-Infinite Context</strong> <br/>
            Hao Liu, Matei Zaharia, Pieter Abbeel <br/>
            <i>International Conference on Learning Representations(ICLR)</i>, 2024 <br/>
            <a href="#" onclick="toggleItem('ring2023'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2310.01889>paper</a> |
            <a href=https://github.com/lhao499/blockwise-parallel-transformer.git>code</a> |
            <a href=https://www.businessinsider.com/google-researcher-ai-models-analyze-millions-of-words-at-once-2023-10>media</a> |
            <a href=https://twitter.com/haoliuhl/status/1709630382457733596>tl;dr</a> </li>
            <div id="ring2023" class="hidden" style="display:none;">
            <pre>
            @article{liu2023ring,
                title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
                author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
                journal={International Conference on Learning Representations(ICLR)},
                year={2024}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Blockwise Parallel Transformer for Large Context Models</strong> <br/>
            Hao Liu, Pieter Abbeel <br/>
            <i>Advances in Neural Information Processing Systems(NeurIPS)</i>(Spotlight Presentation), 2023 <br/>
            <a href="#" onclick="toggleItem('blockwise2023'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2305.19370>paper</a> | <a href=https://github.com/lhao499/blockwise-parallel-transformer.git>code</a> |
            <a href=https://twitter.com/haoliuhl/status/1664396377252667393>tl;dr</a>
            </li>
            <div id="blockwise2023" class="hidden" style="display:none;">
            <pre>
            @article{liu2023blockwise,
                title={Blockwise Parallel Transformer for Large Context Models},
                author={Liu, Hao and Abbeel, Pieter},
                journal={Advances in neural information processing systems},
                year={2023}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment</strong> <br/>
            Hao Liu, Wilson Yan, Pieter Abbeel <br/>
            <i>Advances in Neural Information Processing Systems(NeurIPS)</i>, 2023 <br/>
            <a href="#" onclick="toggleItem('lqae2023'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2302.00902>paper</a> | <a href=https://github.com/lhao499/lqae>code</a> |
            <a href=https://twitter.com/haoliuhl/status/1625273748629901312>tl;dr</a>
            </li>
            <div id="lqae2023" class="hidden" style="display:none;">
            <pre>
            @article{liu2023language,
                title={Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment},
                author={Liu, Hao and Yan, Wilson and Abbeel, Pieter},
                journal={Advances in neural information processing systems},
                year={2023}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Chain of Hindsight Aligns Language Models with Feedback</strong> <br/>
            Hao Liu, Carmelo Sferrazza, Pieter Abbeel <br/>
            <i>International Conference on Learning Representations(ICLR)</i>, 2024 <br/>
            <a href="#" onclick="toggleItem('coh2023'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2302.02676>paper</a> | <a href=https://github.com/lhao499/CoH>code</a> |
            <a href=https://twitter.com/haoliuhl/status/1630696378325413888>tl;dr</a> </li>
            <div id="coh2023" class="hidden" style="display:none;">
            <pre>
            @article{liu2023chain,
                title={Chain of hindsight aligns language models with feedback},
                author={Liu, Hao and Sferrazza, Carmelo and Abbeel, Pieter},
                journal={International Conference on Learning Representations(ICLR)},
                year={2024}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Emergent Agentic Transformer from Chain of Hindsight Experience</strong> <br/>
            Hao Liu, Pieter Abbeel <br/>
            <i>International Conference on Machine Learning(ICML)</i>, 2023 <br/>
            <a href="#" onclick="toggleItem('eate2023'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2305.16554>paper</a> |
            <a href=https://twitter.com/haoliuhl/status/1683319663651061760>tl;dr</a>
            </li>
            <div id="eate2023" class="hidden" style="display:none;">
            <pre>
            @inproceedings{liu2023emergent,
                title={Emergent Agentic Transformer from Chain of Hindsight Experience},
                author={Liu, Hao and Abbeel, Pieter},
                booktitle={International Conference on Machine Learning},
                year={2023}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Exploration with Principles for Diverse AI Supervision</strong> <br/>
            Hao Liu, Matei Zaharia, Pieter Abbeel <br/>
            <i>Preprint</i>, 2023 <br/>
            <a href="#" onclick="toggleItem('exploration2023'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2310.08899>paper</a>
            <div id="ring2023" class="hidden" style="display:none;">
            <pre>
            @article{exploration2023,
                title={Exploration with Principles for Diverse AI Supervision},
                author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
                journal={arXiv preprint arXiv:2310.08899},
                year={2023}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Koala: A dialogue model for academic research</strong> <br/>
            Xinyang Geng*, Arnav Gudibande*, Hao Liu*, Eric Wallace*, Pieter Abbeel†, Sergey Levine†, Dawn Song†. <br/>
            <i>Blog</i>, 2023 <br/>
            <a href="#" onclick="toggleItem('koala2023'); return false;">bib</a> |
            <a href=https://bair.berkeley.edu/blog/2023/04/03/koala>blog</a> </li>
            <div id="koala2023" class="hidden" style="display:none;">
            <pre>
            @article{geng2023koala,
                title={Koala: A dialogue model for academic research},
                author={Geng, Xinyang and Gudibande, Arnav and Liu, Hao and Wallace, Eric and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
                journal={Blog post, April},
                volume={1},
                year={2023}
            }
            </pre> </div>
    
            <li class="paper"> <strong>OpenLLaMa, an open reproduction of LLaMA</strong> <br/>
            Xinyang Geng*, Hao Liu*. <br/>
            <i>GitHub</i>, 2023 <br/>
            <a href="#" onclick="toggleItem('openlm2023openllama'); return false;">bib</a> |
            <a href=https://github.com/openlm-research/open_llama>code</a> |
            <a href=https://twitter.com/haoliuhl/status/1653460935200346113>tl;dr</a> </li>
            <div id="openlm2023openllama" class="hidden" style="display:none;">
            <pre>
            @software{openlm2023openllama,
                author = {Geng, Xinyang and Liu, Hao},
                title = {OpenLLaMA: An Open Reproduction of LLaMA},
                month = May,
                year = 2023,
                url = {https://github.com/openlm-research/open_llama}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Masked Autoencoding for Scalable and Generalizable Decision Making</strong> <br/>
            Fangchen Liu*, Hao Liu*, Aditya Grover, Pieter Abbeel <br/>
            <i>Advances in Neural Information Processing Systems(NeurIPS)</i>, 2022 <br/>
            <a href="#" onclick="toggleItem('maskdp2022'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2211.12740>paper</a> | [<a
            href=https://github.com/FangchenLiu/MaskDP_public>code</a> |
            <a href=https://twitter.com/haoliuhl/status/1597953615162843137>tl;dr</a> </li>
            <div id="maskdp2022" class="hidden" style="display:none;">
            <pre>
            @inproceedings{liu2022masked,
                title={Masked Autoencoding for Scalable and Generalizable Decision Making},
                author={Liu, Fangchen and Liu, Hao and Grover, Aditya and Abbeel, Pieter},
                booktitle={Advances in Neural Information Processing Systems},
                year={2022}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning</strong> <br/>
            Denis Yarats*, David Brandfonbrener*, Hao Liu, Michael Laskin, Pieter Abbeel, Alessandro Lazaric, Lerrel Pinto <br/>
            <i>Arxiv</i>, 2022 <br/>
            <a href="#" onclick="toggleItem('yarats2022don'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2201.13425>paper</a> | <a href=https://sites.google.com/view/exorl>code</a> |
            <a href=https://twitter.com/denisyarats/status/1491813356176609287>tl;dr</a> </li>
            </li>
            <div id="yarats2022don" class="hidden" style="display:none;">
            <pre>
            @article{yarats2022don,
                title={Don't change the algorithm, change the data: Exploratory data for offline reinforcement learning},
                author={Yarats, Denis and Brandfonbrener, David and Liu, Hao and Laskin, Michael and Abbeel, Pieter and Lazaric, Alessandro and Pinto, Lerrel},
                journal={arXiv preprint arXiv:2201.13425},
                year={2022}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Multimodal Masked Autoencoders Learn Transferable Representations</strong> <br/>
            Xinyang Geng*, Hao Liu*, Lisa Lee, Dale Schuurmans, Sergey Levine, Pieter Abbeel <br/>
            <i>ICML Pre-training Workshop</i> (Oral Presentation), 2022. <br/>
            <a href="#" onclick="toggleItem('m3ae2022'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2205.14204>paper</a> | <a href=https://github.com/young-geng/m3ae_public>code</a> |
            <a href=https://twitter.com/haoliuhl/status/1531742479233404928>tl;dr</a> </li>
            </li>
            <div id="m3ae2022" class="hidden" style="display:none;">
            <pre>
            @article{geng2022multimodal,
                title={Multimodal Masked Autoencoders Learn Transferable Representations},
                author={Geng, Xinyang and Liu, Hao and Lee, Lisa and Schuurmans, Dale and Levine, Sergey and Abbeel, Pieter},
                journal={arXiv preprint arXiv:2205.14204},
                year={2022}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Palm up: Playing in the Latent Manifold for Unsupervised Pretraining</strong> <br/>
            Hao Liu, Tom Zahavy, Volodymyr Mnih, Satinder Singh. <br/>
            <i>Advances in Neural Information Processing Systems(NeurIPS)</i>, 2022 <br/>
            <a href="#" onclick="toggleItem('palmup2022'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2210.10913>paper</a> |
            <a href=https://twitter.com/haoliuhl/status/1598331118423375872>tl;dr</a> </li>
            </li>
            <div id="palmup2022" class="hidden" style="display:none;">
            <pre>
            @inproceedings{liu2022palm,
                title={Palm up: Playing in the Latent Manifold for Unsupervised Pretraining},
                author={Liu, Hao and Zahavy, Tom and Mnih, Volodymyr and Singh, Satinder},
                booktitle={Advances in Neural Information Processing Systems},
                year={2022}
            }
            </pre> </div>
    
            <li class="paper"> <strong>URLB: Unsupervised Reinforcement Learning Benchmark. </strong><br/>
            Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang, <br/> Lerrel Pinto, Pieter Abbeel <br/>
            <i>NeurIPS 2021 Track Datasets and Benchmarks</i>, 2021 <br/>
            <a href="#" onclick="toggleItem('urlb2021'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2110.15191>paper</a> | <a href=https://github.com/rll-research/url_benchmark>code</a> |
            <a href=https://twitter.com/denisyarats/status/1491813356176609287>tl;dr</a> </li>
            </li>
            <div id="urlb2021" class="hidden" style="display:none;">
            <pre>
            @article{laskin2021urlb,
                title={URLB: Unsupervised Reinforcement Learning Benchmark},
                author={Laskin, Michael and Yarats, Denis and Liu, Hao and Lee, Kimin and Zhan, Albert and Lu, Kevin and Cang, Catherine and Pinto, Lerrel and Abbeel, Pieter},
                journal={arXiv preprint arXiv:2110.15191},
                year={2021}
            }
            </pre> </div>
    
    
            <li class="paper"> <strong>APS: Active Pre-Training with Successor Features</strong> <br/>
            Hao Liu, Pieter Abbeel <br/>
            <i>International Conference on Machine Learning(ICML)</i>(Long Oral Presentation), 2021. <br/>
            <a href="#" onclick="toggleItem('aps2021'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2108.13956>paper</a> | <a href=https://github.com/rll-research/url_benchmark>code</a> </li>
            <div id="aps2021" class="hidden" style="display:none;">
            <pre>
            @inproceedings{liu2021aps,
                title={APS: Active Pre-Training with Successor Features},
                author={Liu, Hao and Abbeel, Pieter},
                booktitle={International Conference on Machine Learning},
                year={2021}
            }
            </pre> </div>
    
            <li class="paper"> <strong>Behavior From the Void: Unsupervised Active Pre-Training</strong> <br/>
            Hao Liu, Pieter Abbeel <br/>
            <i>Advances in Neural Information Processing Systems(NeurIPS)</i>(Spotlight Presentation), 2021.<br/>
            <a href="#" onclick="toggleItem('bfv2021'); return false;">bib</a> |
            <a href=https://arxiv.org/abs/2103.04551>paper</a> | <a href=https://github.com/rll-research/url_benchmark>code</a> |
            <a href=https://twitter.com/haoliuhl/status/1369708005130838016>tl;dr</a> </li>
            <div id="bfv2021" class="hidden" style="display:none;">
            <pre>
            @inproceedings{liu2021behavior,
                title={Behavior From the Void: Unsupervised Active Pre-Training},
                author={Liu, Hao and Abbeel, Pieter},
                booktitle={Advances in Neural Information Processing Systems},
                year={2021}
            }
            </pre> </div>
    
        </div>
    
        <div class="section">
            <span class="header">Education / Experience</span>
            <ul>
                <li>2024: Part-time at Google DeepMind &nbsp;
                </li>
                <li>2023: Part-time at Google DeepMind &nbsp;
                </li>
                <li>2022: Part-time at Google Brain &nbsp;
                </li>
                <li>2021: Intern at DeepMind &nbsp;
                </li>
                <li>2019: Graduate student at UC Berkeley &nbsp;
                </li>
                <li>2018: B.E. from UESTC &nbsp;
                </li>
                <li>2018: Intern at Salesforce Research &nbsp;
                </li>
                <li>2017: Research visit at UT Austin &nbsp;
                </li>
                <li>2017: Research visit at Dartmouth &nbsp;
                </li>
            </ul>
        </div>
    
        <div class="section">
            <span class="header">Teaching and Service</span>
            <ul>
                <li>CS182 / 282A Designing, Visualizing and Understanding Deep Neural Networks, Spring 2022</li>
                <li>CS294-158 Deep Unsupervised Learning, Spring 2024</li>
                <li>Program committee / reviewer for NeurIPS, ICML, ICLR, JMLR, ACL, EMNLP </li>
            </ul>
        </div>
    
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-FQB34P5CDW"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());
            gtag('config', 'G-FQB34P5CDW');
        </script>
    
     -->
    </body>

